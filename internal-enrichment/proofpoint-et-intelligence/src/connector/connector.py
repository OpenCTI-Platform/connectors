import asyncio
from datetime import datetime, time
from typing import Any

from connector.models import (
    AsnParameterModel,
    AsnResponseModel,
    DomainParameterModel,
    DomainResponseModel,
    FileDetailsParameterModel,
    FileDetailsResponseModel,
    FileParameterModel,
    FileResponseModel,
    GeolocationParameterModel,
    GeolocationResponseModel,
    Ipv4ParameterModel,
    Ipv4ResponseModel,
    ReputationParameterModel,
    ReputationResponseModel,
)
from connector.services.client_api import ProofpointEtIntelligenceClient
from connector.services.config_variables import ProofpointEtIntelligenceConfig
from connector.services.converter_to_stix import ConverterToStix
from connector.services.utils import DateTimeFormat, Utils
from pycti import OpenCTIConnectorHelper
from pydantic import ValidationError
from tenacity import RetryError


class ProofpointEtIntelligenceConnector:
    """Specifications of the internal enrichment connector

    This class encapsulates the main actions, expected to be run by any internal enrichment connector.
    Note that the attributes defined below will be complemented per each connector type.
    This type of connector aim to enrich a data (Observables) created or modified in the OpenCTI core platform.
    It will create a STIX bundle and send it in a RabbitMQ queue.
    The STIX bundle in the queue will be processed by the workers.
    This type of connector uses the basic methods of the helper.
    Ingesting a bundle allow the connector to be compatible with the playbook automation feature.

    Attributes
        self.config (ProofpointEtIntelligenceConfig()):
            Initialize the connector with necessary configuration environment variables

        self.helper (OpenCTIConnectorHelper(config)):
            This is the helper to use.
            ALL connectors have to instantiate the connector helper with configurations.
            Doing this will do a lot of operations behind the scene.

        self.client (ProofpointEtIntelligenceClient(self.helper, self.config):

        self.converter_to_stix (ConnectorConverter(helper)):
            Provide methods for converting various types of input data into STIX 2.1 objects.

        self.stix_objects (list):

    Notes
        - self.helper.connector_logger.[info/debug/warning/error] is used when logging a message
        - self.helper.stix2_create_bundle(stix_objects) is used when creating a bundle
        - self.helper.send_stix2_bundle(stix_objects_bundle) is used to send the bundle to RabbitMQ
    """

    def __init__(self):
        """Initialize the Connector with necessary configurations"""

        # Load configuration file and connection helper
        self.config = ProofpointEtIntelligenceConfig()
        # playbook_compatible=True only if a bundle is sent !
        self.helper = OpenCTIConnectorHelper(
            config=self.config.load, playbook_compatible=True
        )
        self.client = ProofpointEtIntelligenceClient(self.helper, self.config)
        self.converter_to_stix = ConverterToStix(self.helper)
        self.utils = Utils()

        self.stix_objects = None
        self.is_playbook = None

    def _process_submission(
        self, entity_type: str, entity_value: str, stix_object: list[dict]
    ) -> int:
        """

        Args:
            entity_type:
            entity_value:
            stix_object:

        Returns:

        """

        work_id = self._initiate_work(entity_type, entity_value)
        bundle_sent = self._send_intelligence(
            stix_object, (None if self.is_playbook else work_id)
        )
        self._complete_work(entity_type, entity_value, work_id)

        return bundle_sent

    def _initiate_work(self, entity_type: str, entity_value: str) -> str:
        """
        Starts a work enrichment process. only one work per entity.
        Sends a request to the API with the initiate_work method to initialize the work.
        Args:
            entity_type (str): The type of entity being processed ("IPv4-Addr","Domain-Name" or "StixFile").
            entity_value (str) :
        Returns:
            str: The work ID generated by the API when initiating the work stix_entity.
        """
        now_isoformat = self.utils.get_now(DateTimeFormat.ISO)
        enrichment_type = "Playbook" if self.is_playbook else "Manual/Auto"
        self.helper.connector_logger.info(
            "[CONNECTOR] Starting work enrichment...",
            {
                "enrichment_type": enrichment_type,
                "entity_type": entity_type,
                "entity_value": entity_value,
                "isoformat": now_isoformat,
            },
        )

        friendly_name = (
            f"{enrichment_type} - {entity_type} ({entity_value}) run @ {now_isoformat}"
        )
        return self.helper.api.work.initiate_work(self.helper.connect_id, friendly_name)

    def _send_intelligence(self, stix_objects: list[dict], work_id: str = None) -> int:
        """Send the transformed intelligence data to OpenCTI.

        This method prepares and sends unique STIX objects to OpenCTI.
        This method takes a list of objects prepared by the models, extracts their STIX representations, creates a serialized STIX bundle and It then sends this bundle to OpenCTI.
        If prepared objects exist, the method ensures that only unique objects with an 'id' attribute are included. After sending the STIX objects, it keeps inform of the number of bundles sent.
        Args:
            work_id (str): The unique identifier for the work process associated with the STIX objects.
            stix_objects (list): A list of objects containing STIX representations to be sent to OpenCTI.
        Returns:
            None
        """
        self.helper.connector_logger.info(
            "[CONNECTOR] Start sending data for ProofPoint ET Intelligence..."
        )

        stix_objects_bundle = self.helper.stix2_create_bundle(stix_objects)
        bundles_sent = self.helper.send_stix2_bundle(
            stix_objects_bundle,
            work_id=work_id,
            cleanup_inconsistent_bundle=True,
        )
        len_bundle_sent = len(bundles_sent)
        self.helper.connector_logger.info(
            "[CONNECTOR] Sending STIX objects to OpenCTI...",
            {"bundles_sent": len_bundle_sent},
        )
        return len_bundle_sent

    def _complete_work(self, entity_type: str, entity_value: str, work_id: str) -> None:
        """
        Marks the work process as complete.
        This method logs the completion of the work for a specific work ID.
        Sends a request to the API with the to_processed method to complete the work.
        Args:
            entity_type (str): Enriched entity type ("IPv4-Addr", "Domain-Name", "StixFile").
            work_id (str): The unique identifier of the work to mark as complete.
        Returns:
            None
        """
        self.helper.connector_logger.info(
            "[CONNECTOR] Complete enrichment entity...",
            {
                "entity_type": entity_type,
                "entity_value": entity_value,
                "work_id": work_id,
            },
        )
        message = "ProofPoint ET Intelligence - Finished work"
        self.helper.api.work.to_processed(work_id, message)

    def _classify_results(self, results: dict) -> dict:
        successful_calls = {}
        unsuccessful_calls = {}

        for task_name, result in results.items():
            if isinstance(result, RetryError):
                inner_exception = result.last_attempt.exception()
                inner_exception_type = type(inner_exception).__name__

                if inner_exception_type == "ClientResponseError":
                    unsuccessful_calls.update(
                        {
                            "task_name": task_name,
                            "message": "[CONNECTOR-API] A HTTP error occurred during data recovery",
                            "error": inner_exception,
                            "error_type": inner_exception_type,
                            "status": inner_exception.status,
                            "url": inner_exception.request_info.url,
                        }
                    )
                elif inner_exception_type == "ClientConnectionError":
                    unsuccessful_calls.update(
                        {
                            "task_name": task_name,
                            "message": "[CONNECTOR-API] A connection error occurred during data recovery",
                            "error": inner_exception,
                            "error_type": inner_exception_type,
                        }
                    )
                else:
                    unsuccessful_calls.update(
                        {
                            "task_name": task_name,
                            "message": "[CONNECTOR-API] An unexpected error occurred during the recovery of all data",
                            "error": inner_exception,
                            "error_type": inner_exception_type,
                        }
                    )
            else:
                successful_calls.update({task_name: result})

        for call in unsuccessful_calls.items():
            self.helper.connector_logger.error(
                "[ERROR] Unsuccessful call from ProofPoint ET Intelligence",
                {"details": call},
            )

        return successful_calls

    async def _collect_intelligence_file(
        self, entity_value: str, source_entity_type: str
    ) -> dict:
        """

        Args:
            entity_value:
            source_entity_type:

        Returns:

        """
        try:
            tasks = {
                "get_details": self.client.get_details(
                    entity_value, source_entity_type
                ),
            }
            raw_results = await asyncio.gather(*tasks.values(), return_exceptions=True)
            # Reassign keys by task name
            results = {
                task_name: result
                for task_name, result in zip(tasks.keys(), raw_results)
            }
            successful_calls = self._classify_results(results)

            return successful_calls
        except Exception as err:
            self.helper.connector_logger.error(
                "[ERROR] An unexpected error has occurred while collecting intelligence - File.",
                {"error": err},
            )
            raise

    async def _collect_intelligence_domain(
        self, entity_value: str, source_entity_type: str
    ) -> dict:
        """

        Args:
            entity_value:
            source_entity_type:

        Returns:

        """
        try:
            tasks = {
                "get_reputation": self.client.get_reputation(
                    entity_value, source_entity_type
                ),
                "get_ips": self.client.get_ips(entity_value, source_entity_type),
                "get_malwares": self.client.get_malwares(
                    entity_value, source_entity_type
                ),
            }
            raw_results = await asyncio.gather(*tasks.values(), return_exceptions=True)
            # Reassign keys by task name
            results = {
                task_name: result
                for task_name, result in zip(tasks.keys(), raw_results)
            }
            successful_calls = self._classify_results(results)

            return successful_calls
        except Exception as err:
            self.helper.connector_logger.error(
                "[ERROR] An unexpected error has occurred while collecting intelligence - Domain.",
                {"error": err},
            )
            raise

    async def _collect_intelligence_ipv4(
        self, entity_value: str, source_entity_type: str
    ) -> dict:
        """

        Args:
            entity_value:
            source_entity_type:

        Returns:

        """
        try:
            tasks = {
                "get_reputation": self.client.get_reputation(
                    entity_value, source_entity_type
                ),
                "get_domains": self.client.get_domains(
                    entity_value, source_entity_type
                ),
                "get_malwares": self.client.get_malwares(
                    entity_value, source_entity_type
                ),
                "get_geolocation": self.client.get_geolocation(
                    entity_value, source_entity_type
                ),
                "get_asn": self.client.get_asn(entity_value, source_entity_type),
            }
            raw_results = await asyncio.gather(*tasks.values(), return_exceptions=True)
            # Reassign keys by task name
            results = {
                task_name: result
                for task_name, result in zip(tasks.keys(), raw_results)
            }
            successful_calls = self._classify_results(results)

            return successful_calls
        except Exception as err:
            self.helper.connector_logger.error(
                "[ERROR] An unexpected error has occurred while collecting intelligence - IPv4.",
                {"error": err},
            )
            raise

    def _process_collect_intelligence(
        self, entity_type: str, entity_value: str
    ) -> dict:
        """

        Args:
            entity_type:
            entity_value:

        Returns:

        """
        try:
            self.helper.connector_logger.info(
                "[CONNECTOR] Starts collecting data for ProofPoint ET Intelligence"
            )

            entity_type_map = {
                "file": lambda: self._collect_intelligence_file(
                    entity_value, "samples"
                ),
                "domain-name": lambda: self._collect_intelligence_domain(
                    entity_value, "domains"
                ),
                "ipv4-addr": lambda: self._collect_intelligence_ipv4(
                    entity_value, "ips"
                ),
            }

            method_found = entity_type_map.get(entity_type)
            if method_found:
                collected_intelligence = asyncio.run(method_found())
            else:
                collected_intelligence = {}

            self.helper.connector_logger.info(
                "[CONNECTOR] Finalisation of the collecting of intelligence from ProofPoint ET Intelligence."
            )
            return collected_intelligence
        except Exception as err:
            self.helper.connector_logger.error(
                "[ERROR] An unexpected error has occurred during intelligence collection.",
                {"error": err},
            )
            raise

    def _process_prepare_intelligence(
        self, stix_entity: dict, collected_intelligence: dict
    ) -> list[dict]:
        """
        Verify, transform and prepare the collected intelligence data.
        Args:
            collected_intelligence:

        Returns:

        Notes:
            1. Validate collected intelligence with models
            2. Aggregate collected intelligence
            3. Transform collected intelligence to STIX 2.1 format
        """
        try:
            self.helper.connector_logger.info(
                "[CONNECTOR] Starts preparing data for ProofPoint ET Intelligence..."
            )

            validated_intelligence = self._valid_intelligence(collected_intelligence)
            aggregated_and_filtered_intelligence = (
                self._aggregate_and_filter_intelligence(validated_intelligence)
            )
            transformed_intelligence = self._transform_intelligence(
                stix_entity, aggregated_and_filtered_intelligence
            )

            self.helper.connector_logger.info(
                "[CONNECTOR] Finalisation of the preparing of intelligence from ProofPoint ET Intelligence."
            )
            return transformed_intelligence

        except Exception as err:
            self.helper.connector_logger.error(
                "[ERROR] An unexpected error has occurred during intelligence preparation.",
                {"error": err},
            )
            raise

    def _valid_intelligence(self, collected_intelligence: dict) -> dict:
        """

        Args:
            collected_intelligence:

        Returns:

        """
        try:
            self.helper.connector_logger.info(
                "[CONNECTOR] Starts validating intelligence from ProofPoint ET Intelligence."
            )

            valided_intelligence = {}

            task_models = {
                "get_asn": (AsnResponseModel, AsnParameterModel),
                "get_details": (FileDetailsResponseModel, FileDetailsParameterModel),
                "get_domains": (DomainResponseModel, DomainParameterModel),
                "get_geolocation": (
                    GeolocationResponseModel,
                    GeolocationParameterModel,
                ),
                "get_ips": (Ipv4ResponseModel, Ipv4ParameterModel),
                "get_malwares": (FileResponseModel, FileParameterModel),
                "get_reputation": (ReputationResponseModel, ReputationParameterModel),
            }

            for task_name, results in collected_intelligence.items():

                if task_name not in task_models:
                    continue

                task_response_model, task_parameter_model = task_models[task_name]
                try:
                    validated_model = task_response_model.model_validate(results)

                    if isinstance(validated_model.payload, list):
                        validated_entries = []

                        for item in validated_model.payload:
                            try:
                                validated_item = task_parameter_model.model_validate(
                                    item
                                )
                                validated_entries.append(validated_item.model_dump())
                            except ValidationError as err:
                                self.helper.connector_logger.warning(
                                    "[VALIDATION] A validation error has been encountered, this validation will be skipped.",
                                    {
                                        "task_name": task_name,
                                        "source_error": item,
                                        "error": err,
                                    },
                                )
                                continue

                        if validated_entries:
                            valided_intelligence[task_name] = validated_entries
                        else:
                            valided_intelligence[task_name] = None

                    elif isinstance(
                        validated_model.payload,
                        (AsnParameterModel | FileDetailsParameterModel),
                    ):
                        try:
                            validated_item = task_parameter_model.model_validate(
                                validated_model.payload
                            )
                            valided_intelligence[task_name] = (
                                validated_item.model_dump()
                            )
                        except ValidationError as err:
                            self.helper.connector_logger.warning(
                                "[VALIDATION] A validation error has been encountered, this validation will be skipped.",
                                {
                                    "task_name": task_name,
                                    "source_error": validated_model.payload,
                                    "error": err,
                                },
                            )
                            continue
                    else:
                        self.helper.connector_logger.warning(
                            "[SKIPPED] An unknown source has been encountered, this validation will be skipped.",
                            {
                                "task_name": task_name,
                                "source.": validated_model.payload,
                            },
                        )
                        continue

                except ValidationError as err:
                    self.helper.connector_logger.warning(
                        "[VALIDATION] A validation error has been encountered for the response structure.",
                        {"task_name": task_name, "error": err},
                    )
                    continue

            self.helper.connector_logger.info(
                "[CONNECTOR] Finalisation of the validation of information from ProofPoint ET Intelligence."
            )
            return valided_intelligence

        except Exception as err:
            self.helper.connector_logger.error(
                "[ERROR] An unexpected error has occurred during intelligence validation.",
                {"error": err},
            )
            raise

    def _aggregate_and_filter_intelligence(self, validated_intelligence: dict) -> dict:
        """

        Args:
            validated_intelligence:

        Returns:

        """
        try:
            self.helper.connector_logger.info(
                "[CONNECTOR] Starts aggregating and filter data from ProofPoint ET Intelligence."
            )
            aggregate_intelligence = {}

            # We calculate the deadline according to the time window chosen by the user.
            limit_date = (
                self.utils.get_now(DateTimeFormat.DATETIME)
                - self.config.extra_import_last_seen_time_window
            )

            # Filters out entities whose last activity date (`last_seen`) is later than `limit_date`.
            # Used for domains, malwares, ips
            filter_by_last_seen = lambda entities: [
                entity
                for entity in entities
                if entity.get("last_seen") and entity["last_seen"] >= limit_date.date()
            ]

            intelligence_mapping = {
                "get_reputation": lambda data: [item.get("category") for item in data],
                "get_domains": filter_by_last_seen,
                "get_ips": filter_by_last_seen,
                "get_malwares": filter_by_last_seen,
                "get_geolocation": lambda data: data,
                "get_asn": lambda data: data,
                "get_details": lambda data: data,
            }

            for key, aggregate_func in intelligence_mapping.items():
                all_data = validated_intelligence.get(key)
                if all_data is not None:
                    aggregate_intelligence[key] = aggregate_func(all_data)

            self.helper.connector_logger.info(
                "[CONNECTOR] Finalisation of the aggregating data from ProofPoint ET Intelligence."
            )
            return aggregate_intelligence

        except Exception as err:
            self.helper.connector_logger.error(
                "[ERROR] An unexpected error has occurred during intelligence aggregation and filter.",
                {"error": err},
            )
            raise

    def _transform_intelligence(
        self, stix_entity: dict, aggregated_intelligence: dict
    ) -> list[dict]:
        """

        Args:
            aggregated_intelligence:

        Returns:

        """
        try:
            self.helper.connector_logger.info(
                "[CONNECTOR] Starts transforming intelligence to STIX 2.1 format."
            )
            stix_objects: list[Any] = []

            # Make Author object
            author = self.converter_to_stix.make_author()
            stix_objects.append(author)

            # Make Markings object
            markings = self.converter_to_stix.make_marking_definition_tlp_amber_strict()
            stix_objects.append(markings)

            entity_type = stix_entity.get("type").lower()
            entity_mapping = {
                "ipv4-addr": self.converter_to_stix.make_ip,
                "domain-name": self.converter_to_stix.make_domain,
                "file": self.converter_to_stix.make_file,
            }

            if entity_type in entity_mapping and aggregated_intelligence:
                extra_parameters: dict | list = aggregated_intelligence.get(
                    "get_details", aggregated_intelligence.get("get_reputation", [])
                )
                make_observable_triggered = entity_mapping[entity_type](
                    True, stix_entity, extra_parameters
                )
                stix_objects.append(make_observable_triggered)
            else:
                self.helper.connector_logger.warning(
                    "[WARNING] Bad scope or no data found"
                )
                return []

            entities_mapping = {
                "get_ips": self.converter_to_stix.make_ip,
                "get_domains": self.converter_to_stix.make_domain,
                "get_malwares": self.converter_to_stix.make_file,
                "get_geolocation": self.converter_to_stix.make_location,
                "get_asn": self.converter_to_stix.make_asn,
            }

            for key, make_entity_func in entities_mapping.items():
                entities: list[dict] | dict = aggregated_intelligence.get(key, [])
                if entities and key in ["get_asn"]:
                    entities: list[dict] = [entities]

                for entity in entities:
                    if key == "get_geolocation":
                        entity = self.utils.get_location_info(entity)

                    prepare_args = (
                        (
                            False,
                            entity,
                        )
                        if key in ["get_ips", "get_domains", "get_malwares"]
                        else (entity,)
                    )
                    make_entity = make_entity_func(*prepare_args)
                    stix_objects.append(make_entity)

                    date = entity.get("first_seen")
                    convert_to_datetime = (
                        datetime.combine(date, time(00, 00, 00)) if date else None
                    )

                    relationship_mapping = {
                        # get_ips : Domain -> "resolve-to" -> IP
                        "get_ips": (
                            make_observable_triggered,
                            "resolves-to",
                            make_entity,
                            convert_to_datetime,
                        ),
                        # get_domains : Domain -> "resolve-to" -> IP
                        "get_domains": (
                            make_entity,
                            "resolves-to",
                            make_observable_triggered,
                            convert_to_datetime,
                        ),
                        # get_malware : File -> "communicates-with" -> IP
                        "get_malwares": (
                            make_entity,
                            "communicates-with",
                            make_observable_triggered,
                            convert_to_datetime,
                        ),
                        # get_geolocation : IP -> "located-at" -> Country
                        "get_geolocation": (
                            make_observable_triggered,
                            "located-at",
                            make_entity,
                        ),
                        # get_asn : IP -> "belongs-to" -> ASN
                        "get_asn": (
                            make_observable_triggered,
                            "belongs-to",
                            make_entity,
                        ),
                    }

                    if key in relationship_mapping:
                        prepare_args = relationship_mapping.get(key)
                        make_relationship = self.converter_to_stix.make_relationship(
                            *prepare_args
                        )
                        stix_objects.append(make_relationship)

            # Filters objects to retain only those with a unique ID, preventing duplication in the final list.

            unique_ids = set()
            stix_representation_objects = [
                obj.stix2_representation
                for obj in stix_objects
                if obj.id not in unique_ids and not unique_ids.add(obj.id)
            ]
            self.helper.connector_logger.info(
                "[CONNECTOR] Finalisation of the transforming intelligence to STIX 2.1 format."
            )
            return stix_representation_objects

        except Exception as err:
            self.helper.connector_logger.error(
                "[ERROR] An unexpected error has occurred during intelligence transformation.",
                {"error": err},
            )
            raise

    def _extract_and_check_markings(self, opencti_entity: dict) -> None:
        """Extracts the highest TLP marking from the specified OpenCTI entity and ensures it is within the maximum
        allowed TLP level configured for the connector.

        This method processes the `objectMarking` list from the `opencti_entity`, identifies all TLP markings, and
        retains the highest TLP level based on a predefined hierarchy. If the highest TLP level exceeds the connector's
        configured maximum TLP level, an exception is raised, indicating that enrichment of the entity was unsuccessful.

        Args:
            opencti_entity (dict): A dictionary representing an entity from OpenCTI. The dictionary must include an
            `objectMarking` key, which contains a list of marking definitions, including TLP levels.

        Raises:
            ValueError: If the entity's highest TLP level exceeds the maximum TLP level allowed by the connector
            configuration.

        Returns:
            None: This method does not return any value. It performs a validation check to determine whether the
            entity's TLP level meets the configured requirements.

        Notes :
            - The method defines a hierarchy for TLP levels as follows:
            "TLP:WHITE" < "TLP:CLEAR" < "TLP:GREEN" < "TLP:AMBER" < "TLP:AMBER+STRICT" < "TLP:RED".
        """
        highest_tlp_entity = None
        entity_marking = opencti_entity.get("objectMarking", [])

        ordered_tlp_levels = [
            "TLP:WHITE",
            "TLP:CLEAR",
            "TLP:GREEN",
            "TLP:AMBER",
            "TLP:AMBER+STRICT",
            "TLP:RED",
        ]

        for marking_definition in entity_marking:
            if marking_definition.get("definition_type") != "TLP":
                continue

            current_tlp = marking_definition.get("definition")
            current_tlp_index = ordered_tlp_levels.index(current_tlp)
            highest_tlp_index = (
                ordered_tlp_levels.index(highest_tlp_entity)
                if highest_tlp_entity in ordered_tlp_levels
                else -1
            )
            if current_tlp_index > highest_tlp_index:
                highest_tlp_entity = current_tlp

        valid_max_tlp = self.helper.check_max_tlp(
            highest_tlp_entity, self.config.extra_max_tlp
        )

        if not valid_max_tlp:
            raise ValueError(
                "[CONNECTOR] Enrichment of the entity was unsuccessful, the entity's TLP is greater than the MAX TLP "
                "allowed in the connector's configuration, so it does not have the necessary authorisation to enrich "
                "this entity."
            )

    def _is_entity_in_scope(self, data) -> bool:
        """
        Security to limit playbook triggers to something other than the initial entity scope
        """
        scopes = self.config.connector_scope.lower().split(",")
        entity_type = data.get("x_opencti_type", data.get("type")).lower()

        if entity_type == "stixfile" or entity_type == "file":
            if not "MD5" in data.get("hashes", {}):
                self.helper.connector_logger.debug(
                    "[CONNECTOR] Enrichment of the entity was unsuccessful: the entity of type 'StixFile' or 'File' "
                    "does not contain an 'MD5' hash in the 'hashes' attribute, which is required for "
                    "ProofPoint ET Intelligence."
                )
                return False

        return entity_type in scopes

    def process_message(self, data: dict) -> str:
        """The main process used by the connector to collect intelligence
        This method launches the connector, processes the current state, collects .

        Returns:
            str : messages
        """
        try:
            get_now = self.utils.get_now()
            connector_start_isoformat = get_now.get("now_isoformat")

            self.stix_objects = data.get("stix_objects")
            initial_len_stix_objects = len(self.stix_objects)

            stix_entity = data.get("stix_entity")
            opencti_entity = data.get("enrichment_entity")
            self.is_playbook = False if data.get("event_type") else True

            self._extract_and_check_markings(opencti_entity)

            self.helper.connector_logger.info(
                "[CONNECTOR] Queue listening has been triggered by the connector...",
                {
                    "connector_triggered": connector_start_isoformat,
                    "entity_id": stix_entity.get("id"),
                },
            )

            if self._is_entity_in_scope(stix_entity):
                self.helper.connector_logger.info("[CONNECTOR] Starting enrichment...")
                entity_type = stix_entity.get("type").lower()
                entity_value = stix_entity.get("value") or stix_entity.get(
                    "hashes"
                ).get("MD5")

                # Start collecting data from ProofPoint ET Intelligence
                collected_intelligence = self._process_collect_intelligence(
                    entity_type, entity_value
                )

                if collected_intelligence:
                    # Start preparing data for OpenCTI - Converted to stix format
                    prepared_intelligence = self._process_prepare_intelligence(
                        stix_entity, collected_intelligence
                    )

                    self.stix_objects.extend(prepared_intelligence)

                    len_list_id_unique = len(
                        {item["id"] for item in self.stix_objects if "id" in item}
                    )
                    # (- 2) Because we remove the addition of author and marking definition
                    total_len_stix_objects = len_list_id_unique - 2

                    self.helper.connector_logger.info(
                        "[CONNECTOR] The intelligence preparation process has been completed.",
                        {
                            "entity_id": stix_entity.get("id"),
                            "collected_entities": total_len_stix_objects,
                        },
                    )

                else:
                    total_len_stix_objects = initial_len_stix_objects

                # Start sending data to OpenCTI
                bundle_sent = self._process_submission(
                    entity_type, entity_value, self.stix_objects
                )

                if total_len_stix_objects > initial_len_stix_objects:
                    message = (
                        "The enrichment process has been completed and the number of entities that have been "
                        f"collected by ProofPoint ET Intelligence is as follows: {bundle_sent}"
                    )
                else:
                    message = "[CONNECTOR] Enrichment process has been completed"
                    # Enriching a file adds information to the entity but does not add any new entities.
                    if entity_type != "file":
                        message += (
                            ", but no new information was collected by ProofPoint ET Intelligence. "
                            "Therefore, no enrichment was performed."
                        )

                self.helper.connector_logger.info(
                    message,
                    {"entity_type": entity_type, "entity_id": stix_entity.get("id")},
                )
                return message

            else:
                if self.is_playbook:
                    # If the entity is not in scope and the event_type is not present in the triggered entity,
                    # this indicates a trigger initiated by the playbook.
                    # In this case we return the triggered entity initiated by the playbook.
                    self._send_intelligence(self.stix_objects)
                else:
                    message = (
                        "[CONNECTOR] Skip the following entity as it does not concern "
                        "the initial scope found in the connector config..."
                    )
                    self.helper.connector_logger.info(
                        message, {"entity_id": stix_entity["id"]}
                    )
                    return message

        except Exception as err:
            self.helper.connector_logger.error(
                "[CONNECTOR] Unexpected Error occurred", {"error_message": str(err)}
            )
            raise

    def run(self) -> None:
        """Run the main process in self.helper.listen() method
        The method continuously monitors a message queue associated with a specific connector
        The connector have to listen a specific queue to get and then enrich the information.
        The helper provide an easy way to listen to the events.
        """
        self.helper.listen(message_callback=self.process_message)
